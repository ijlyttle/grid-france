{
  "hash": "855bd4b844c7ddee58eb813a0975cc64",
  "result": {
    "markdown": "---\ntitle: About DVC\nexecute: \n  freeze: auto\n---\n\nDVC is an acronym for \"data version control\" - it's an open-source project. In this context, DVC is helping with three tasks:\n\n-   specifying dependencies within the pipeline, e.g. which Quarto files to render\n-   managing parameters, which can\n-   managing versions of large data files on remote storage\n\nAlthough it runs using Python, it does not require that what it runs use Python. In other words, it's agnostic about languages used in the pipeline.\n\n## Pipeline dependencies\n\nA DVC pipeline includes a series of [stages](https://dvc.org/doc/user-guide/project-structure/dvcyaml-files), the dependencies between the stages form a directed acyclic graph (DAG).\n\nHere's a flowchart for this pipeline's stages, generated using DVC:\n\n```{mermaid}\nflowchart TD\n\tnode1[\"flow-ingest\"]\n\tnode2[\"flow-publish\"]\n\tnode3[\"flow-transform\"]\n\tnode4[\"generation-ingest\"]\n\tnode5[\"generation-publish\"]\n\tnode6[\"generation-transform\"]\n\tnode7[\"index\"]\n\tnode1-->node3\n\tnode2-->node7\n\tnode3-->node2\n\tnode4-->node6\n\tnode5-->node7\n\tnode6-->node5\n\n```\n\n\nStages are defined in a `dvc.yaml` file, here's an exerpt:\n\n``` yaml\nstages:\n\n  # more stages before and after this one\n  flow-transform:\n    cmd: quarto render data-flow-02-transform.qmd\n    params: \n      - dvc-params.yaml:\n        - tz_local\n    deps:\n      - data/01-ingest/flow.json\n      - data-flow-02-transform.qmd\n    outs: \n      - data/02-transform/flow.parquet:\n          persist: true\n```\n\nYou can see that we declare:\n\n-   the command to run, if needed\n-   parameters used\n-   file dependencies\n-   file outputs\n\nEach stage in a pipeline has its own yaml entry. We have to declare each dependency explicitly, independently from the code in the `.qmd` file.\n\n::: callout-note\nAs I understand, this is different from how the R package [**targets**](https://books.ropensci.org/targets/walkthrough.html) works; there the dependencies are deduced from `targets` calls within `.qmd` files. In this sense, the `targets` approach seems less duplicative, less brittle. That said, `targets` is designed for R only.\n:::\n\nAs a result of DVC \"managing\" the dependencies, DVC also \"manages\" the computational part of rendering process. To run the pipeline, you (or GitHub Actions) would:\n\n``` bash\ndvc repro\n```\n\n::: callout-note\nI'll talk about this more in the [Quarto section](about-quarto.qmd): for those who have some experience with Quarto, I specify in `_quarto.yml`:\n\n``` yaml\nexecute: \n  freeze: true\n```\n\nThen, for `.qmd` files, like this one, that are not part of the pipeline, I specify:\n\n``` yaml\nexecute: \n  freeze: auto\n```\n\nUsing the [`freeze` option](https://quarto.org/docs/projects/code-execution.html#freeze) this way, when I (or GitHub Actions) run `quarto render`, the pipeline will not run, but the website will be built.\n:::\n\n## Parameters\n\nDVC offers ways to manage [stage-level parameters](https://dvc.org/doc/user-guide/project-structure/dvcyaml-files#parameters) as part of a pipeline. The `dvc.api` package lets you access parameters from within your `.qmd` file, with a call like:\n\n``` python\nparams = dvc.api.params_show(\"dvc-params.yaml\")\n```\n\nThis is a python package; using it violates the principle of DVC not caring about what language you use in the pipeline. That said, I *think* that you could manage this from within the pipeline by reading the YAML file using the language of your choice. The dependency on the given parameter,`tz_local` in this case, is declared in `dvc.yaml`.\n\nThis is my *first* DVC pipeline, I have used only Python, so `¯\\_(ツ)_/¯ alt=\"shrug\"`.\n\n## Remote storage\n\nThis is the feature that first piqued my interest. DVC supports [remote storage ](https://dvc.org/doc/user-guide/data-management/remote-storage)on AWS S3, Azure Blob Storage, and many other platforms. This makes things easier to share data, in a principled way, among collaborators.\n\nIn this case, I am \"collaborating\" with GitHub Actions to fill the historical data for generation and flow-exchanges for the French electrical grid. Each API call covers two weeks; I have a GitHub Action with a daily schedule-trigger.\n\nYou should consult the DVC documentation to set up your own remote storage. The purpose of this section is to give my impression of how things work, and how DVC remote storage fits into a git-based workflow.\n\nI think of DVC remote storage as \"git-remotes\", but for data. The metadata describing the data that *should* be available is part of a regular git repository. For example:\n\n-   `dvc.lock` contains metadata (hashes, etc.) on all the dependencies and outputs, many of these live in the `data/` directory\n\n-   `.dvc/config` contains information on the remotes (but not the authentication), e.g.:\n\n    ``` gitconfig\n    [core]\n        remote = datastore\n    ['remote \"datastore\"']\n        url = s3://ijlyttle-grid-france\n    ```\n\nThe `data/` directory is largely git-ignored. The metadata on these files is stored in `dvc.lock`, DVC will know what to do.\n\nA workflow might look something like this:\n\n``` bash\ngit pull\ndvc pull\n\ndvc repro # run the pipeline\nquarto render # render the rest of the website\nquarto publish # deploy the website\n\ngit commit -am \"Automated report\"\ngit push\ndvc push\n```\n\nIn this example, we do not use the equivalent of `git add` for DVC, because the pipeline-definition file `dvc.yaml` has taken care of it for us.\n\n## Jardinage\n\nThe French word for \"gardening\", a happier way to refer to maintenance.\n\nEach time the pipeline runs, DVC caches all the files it uses; with remote storage, this could potentially incur non-trivial storage costs. For this pipeline, each run produces a couple of MB - not too bad. At some point, I will like to prune the cache on my remote storage, DVC offers a [clear-cache utility](https://dvc.org/doc/command-reference/gc#cleaning-shared-cache-or-remote); it can filter by date.\n\nMe writing this here is meant as a reminder to myself to get this done, and documented, in the next little while.\n\n## Perspectiive on pins\n\nPins is a set of packages: [pins for R](https://pins.rstudio.com/) and [pins for Python](https://rstudio.github.io/pins-python/?_gl=1*mhjoml*_ga*MzY0NTAzNDEzLjE2OTI5NzgyODE.*_ga_2C0WZ1JHG0*MTY5NDM5MzA4NS43LjAuMTY5NDM5MzA4NS4wLjAuMA..), developed and maintained by Posit. Using pins is where I first started to think about management of remote data-sets. When I started learning about DVC, many of the concepts were already familiar, thanks to pins.\n\nIn my view, there is an important architectural difference between DVC and pins:\n\n-   pins functions are called from the code \"inside\" the pipeline; they are concerned with fetching and pushing remote data.\n\n-   DVC is invoked \"outside\" the pipeline; it puts the pieces into place before and after the pipeline is run.\n\nOf course this distinction ignores the `dvc.api` calls mentioned above. That said, because DVC runs \"outside\" the pipeline, it can be agnostic about the language used within it, so long as Python and the `dvc` package are available.\n\n",
    "supporting": [
      "about-dvc_files"
    ],
    "filters": [],
    "includes": {}
  }
}