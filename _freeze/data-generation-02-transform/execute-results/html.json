{
  "hash": "dac6530f3edc8be8a944740f2933edd7",
  "result": {
    "markdown": "---\ntitle: Generation Transform\n---\n\nIn the **transform** step:\n\n- Parse response, concatenate to existing data, write to parquet files for both \"standard\" and \"fake UTC\" timestamps\n\n\nThe generation data is available from 2017-01-01, onwards.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport json\nfrom datetime import datetime, timedelta\nfrom zoneinfo import ZoneInfo\nimport requests\nimport functools, itertools\nimport polars as pl\nfrom pyprojroot.here import here\n```\n:::\n\n\nLet's determine the `start_date` and the `end_date` for the request:\n\n- the start date will be the larger of:\n\n  - 2017-01-01\n  - the most-recent date in the dataset, less a day\n\n- the end date will be the smaller of:\n  - the start date plus 14 days\n  - the current date\n\nDates are expressed as midnight, Paris time.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ntz_local = ZoneInfo(\"Europe/Paris\")\n```\n:::\n\n\nLet's load in the results from the previous step:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nwith open(here(\"data/01-ingest/generation.json\"), \"r\") as file:\n    array = json.load(file)\n\ntable_existing = pl.read_parquet(here(\"data/99-publish/standard/generation.parquet\"))\n```\n:::\n\n\nHere, I use some functional-programming tools (because that's what I know) to make one observation per interval and production-type:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nby_type = list(\n    map(\n        lambda x: list(\n            map(\n                lambda v: {\n                    \"type\": x[\"production_type\"],\n                    \"interval_start\": v[\"start_date\"],\n                    \"interval_end\": v[\"end_date\"],\n                    \"generation\": v[\"value\"],\n                },\n                x[\"values\"],\n            )\n        ),\n        array,\n    )\n)\n\nfixed = functools.reduce(itertools.chain, by_type)\n```\n:::\n\n\nWe're now in a form to convert this to a Polars DataFrame, to parse, etc.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ntable_raw = pl.DataFrame(fixed)\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ntable = table_raw.with_columns(\n    [\n        pl.col(\"interval_start\")\n        .str.strptime(pl.Datetime(\"ms\"))\n        .dt.convert_time_zone(time_zone=\"Europe/Paris\"),\n        pl.col(\"interval_end\")\n        .str.strptime(pl.Datetime(\"ms\"))\n        .dt.convert_time_zone(time_zone=\"Europe/Paris\"),\n    ]\n)\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ntable.groupby(pl.col(\"type\")).agg(\n    pl.col(\"interval_start\").min(),\n    pl.col(\"interval_end\").max(),\n    pl.col(\"generation\").null_count().alias(\"null_count\"),\n).with_columns(pl.col(\"interval_start\").dt.month().alias(\"month\"))\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div><style>\n.dataframe > thead > tr > th,\n.dataframe > tbody > tr > td {\n  text-align: right;\n}\n</style>\n<small>shape: (10, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>type</th><th>interval_start</th><th>interval_end</th><th>null_count</th><th>month</th></tr><tr><td>str</td><td>datetime[ms, Europe/Paris]</td><td>datetime[ms, Europe/Paris]</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>&quot;FOSSIL_HARD_COâ€¦</td><td>2017-03-26 00:00:00 CET</td><td>2017-04-09 00:00:00 CEST</td><td>0</td><td>3</td></tr><tr><td>&quot;HYDRO&quot;</td><td>2017-03-26 00:00:00 CET</td><td>2017-04-09 00:00:00 CEST</td><td>0</td><td>3</td></tr><tr><td>&quot;WIND&quot;</td><td>2017-03-26 00:00:00 CET</td><td>2017-04-09 00:00:00 CEST</td><td>0</td><td>3</td></tr><tr><td>&quot;NUCLEAR&quot;</td><td>2017-03-26 00:00:00 CET</td><td>2017-04-09 00:00:00 CEST</td><td>0</td><td>3</td></tr><tr><td>&quot;FOSSIL_GAS&quot;</td><td>2017-03-26 00:00:00 CET</td><td>2017-04-09 00:00:00 CEST</td><td>0</td><td>3</td></tr><tr><td>&quot;BIOENERGY&quot;</td><td>2017-03-26 00:00:00 CET</td><td>2017-04-09 00:00:00 CEST</td><td>0</td><td>3</td></tr><tr><td>&quot;EXCHANGE&quot;</td><td>2017-03-26 00:00:00 CET</td><td>2017-04-09 00:00:00 CEST</td><td>0</td><td>3</td></tr><tr><td>&quot;SOLAR&quot;</td><td>2017-03-26 00:00:00 CET</td><td>2017-04-09 00:00:00 CEST</td><td>0</td><td>3</td></tr><tr><td>&quot;PUMPING&quot;</td><td>2017-03-26 00:00:00 CET</td><td>2017-04-09 00:00:00 CEST</td><td>0</td><td>3</td></tr><tr><td>&quot;FOSSIL_OIL&quot;</td><td>2017-03-26 00:00:00 CET</td><td>2017-04-09 00:00:00 CEST</td><td>0</td><td>3</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ntable_combined = (\n    pl.concat([table_existing, table])\n    .unique(subset=[\"interval_start\", \"type\"])\n    .sort(by=[\"interval_start\", \"type\"])\n)\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ntable_combined.write_parquet(here(\"data/02-transform/generation.parquet\"))\n```\n:::\n\n\n",
    "supporting": [
      "data-generation-02-transform_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}