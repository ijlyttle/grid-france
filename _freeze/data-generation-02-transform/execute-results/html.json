{
  "hash": "5cf21b71cdb569c491998374843a8329",
  "result": {
    "markdown": "---\ntitle: Generation Transform\n---\n\nThe goal of this pipeline is to update a parquet file with generation data from the French grid.\n\nIn the **transform** step:\n\n- Parse response saved from the **ingest** step\n- Concatenate with existing parquet files\n- Write parquet files\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport dvc.api\nimport json\nfrom datetime import datetime, timedelta\nfrom zoneinfo import ZoneInfo\nimport requests\nimport functools, itertools\nimport polars as pl\nfrom pyprojroot.here import here\n```\n:::\n\n\nFirst, let's read the parameters:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nparams = dvc.api.params_show(\"dvc-params.yaml\")\n```\n:::\n\n\nLet's load in the results from the previous step. \nWe also load the published results, so that we can append the current results.\nSimilar to the **ingest** step, we do not declare this file as a dependency, to avoid circularity.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nwith open(here(\"data/01-ingest/generation.json\"), \"r\") as file:\n    array = json.load(file)\n\ntable_existing = pl.read_parquet(here(\"data/99-publish/standard/generation.parquet\"))\n```\n:::\n\n\nHere, I use some functional-programming tools (because that's what I know) to make one observation per interval and production-type:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nby_type = list(\n    map(\n        lambda x: list(\n            map(\n                lambda v: {\n                    \"type\": x[\"production_type\"],\n                    \"interval_start\": v[\"start_date\"],\n                    \"interval_end\": v[\"end_date\"],\n                    \"generation\": v[\"value\"],\n                },\n                x[\"values\"],\n            )\n        ),\n        array,\n    )\n)\n\nfixed = functools.reduce(itertools.chain, by_type)\n```\n:::\n\n\nWe're now in a form to convert this to a Polars DataFrame, to parse, etc.\nLet's create a DataFrame from the list:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ntable_raw = pl.DataFrame(fixed)\n```\n:::\n\n\nWe can use Polars expressions to parse the date-times:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ntable = table_raw.with_columns(\n    [\n        pl.col(\"interval_start\")\n        .str.strptime(pl.Datetime(\"ms\"))\n        .dt.convert_time_zone(time_zone=params[\"tz_local\"]),\n        pl.col(\"interval_end\")\n        .str.strptime(pl.Datetime(\"ms\"))\n        .dt.convert_time_zone(time_zone=params[\"tz_local\"]),\n    ]\n)\n```\n:::\n\n\nLet's remove duplicate entries:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ntable_combined = (\n    pl.concat([table_existing, table])\n    .unique(subset=[\"interval_start\", \"type\"])\n    .sort(by=[\"interval_start\", \"type\"])\n)\n```\n:::\n\n\nFor the combined table, for each type of generation, count the observations and null values.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ntable_combined.groupby(pl.col(\"type\")).agg(\n    pl.col(\"interval_start\").min(),\n    pl.col(\"interval_end\").max(),\n    pl.col(\"generation\").count().alias(\"n_observations\"),\n    pl.col(\"generation\").null_count().alias(\"n_value_null\"),\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div><style>\n.dataframe > thead > tr > th,\n.dataframe > tbody > tr > td {\n  text-align: right;\n}\n</style>\n<small>shape: (10, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>type</th><th>interval_start</th><th>interval_end</th><th>n_observations</th><th>n_value_null</th></tr><tr><td>str</td><td>datetime[ms, Europe/Paris]</td><td>datetime[ms, Europe/Paris]</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>&quot;FOSSIL_HARD_COâ€¦</td><td>2017-01-01 00:00:00 CET</td><td>2020-11-01 00:00:00 CET</td><td>134384</td><td>0</td></tr><tr><td>&quot;FOSSIL_GAS&quot;</td><td>2017-01-01 00:00:00 CET</td><td>2020-11-01 00:00:00 CET</td><td>134384</td><td>0</td></tr><tr><td>&quot;NUCLEAR&quot;</td><td>2017-01-01 00:00:00 CET</td><td>2020-11-01 00:00:00 CET</td><td>134384</td><td>0</td></tr><tr><td>&quot;PUMPING&quot;</td><td>2017-01-01 00:00:00 CET</td><td>2020-11-01 00:00:00 CET</td><td>134384</td><td>0</td></tr><tr><td>&quot;BIOENERGY&quot;</td><td>2017-01-01 00:00:00 CET</td><td>2020-11-01 00:00:00 CET</td><td>134384</td><td>0</td></tr><tr><td>&quot;WIND&quot;</td><td>2017-01-01 00:00:00 CET</td><td>2020-11-01 00:00:00 CET</td><td>134384</td><td>0</td></tr><tr><td>&quot;EXCHANGE&quot;</td><td>2017-01-01 00:00:00 CET</td><td>2020-11-01 00:00:00 CET</td><td>134384</td><td>0</td></tr><tr><td>&quot;HYDRO&quot;</td><td>2017-01-01 00:00:00 CET</td><td>2020-11-01 00:00:00 CET</td><td>134383</td><td>0</td></tr><tr><td>&quot;SOLAR&quot;</td><td>2017-01-01 00:00:00 CET</td><td>2020-11-01 00:00:00 CET</td><td>134384</td><td>0</td></tr><tr><td>&quot;FOSSIL_OIL&quot;</td><td>2017-01-01 00:00:00 CET</td><td>2020-11-01 00:00:00 CET</td><td>134384</td><td>0</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nFinally, write the result:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ntable_combined.write_parquet(here(\"data/02-transform/generation.parquet\"))\n```\n:::\n\n\n",
    "supporting": [
      "data-generation-02-transform_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}