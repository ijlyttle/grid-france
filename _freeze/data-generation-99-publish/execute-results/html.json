{
  "hash": "05c94f38d7376a4714237be8749813ce",
  "result": {
    "markdown": "---\ntitle: Generation Publish\n---\n\nIn the **publish** step\n\n- Add to top-level `resources:` in YAML\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport polars as pl\nfrom pyprojroot.here import here\nimport json\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ntable = pl.read_parquet(here(\"data/02-transform/generation.parquet\"))\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ntable_fake_utc = table.with_columns(\n    pl.col([\"interval_start\", \"interval_end\"]).map(\n        lambda x: x.dt.replace_time_zone(time_zone=\"UTC\")\n    ),\n)\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\npath_standard = here(\"data/99-publish/standard\")\nos.makedirs(path_standard, exist_ok=True)\ntable.write_parquet(f\"{path_standard}/generation.parquet\")\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\npath_fake_utc = here(\"data/99-publish/fake-utc\")\nos.makedirs(path_fake_utc, exist_ok=True)\ntable_fake_utc.write_parquet(f\"{path_fake_utc}/generation.parquet\")\n```\n:::\n\n\n## Metadata\n\nSome metadata:\n\n - `timezone`: should be `\"Europe/Paris\"`\n - `interval_end_minimum`: aggregated over type, last \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ntime_zone = table.get_column(\"interval_start\").dtype.time_zone\n```\n:::\n\n\nFor each type, find the latest `interval_end` - then find the earliest among those; this is where the next query will need to start.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ninterval_end = (\n    table.groupby(pl.col(\"type\"))\n    .agg(pl.col(\"interval_end\").max())\n    .get_column(\"interval_end\")\n    .min()\n)\n```\n:::\n\n\nWe write this to a metadata file:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndict = {\"time_zone\": time_zone, \"interval_end\": interval_end.isoformat()}\n\nwith open(here(\"data/99-publish/generation-meta.json\"), \"w\") as file:\n    json.dump(dict, file)\n```\n:::\n\n\n",
    "supporting": [
      "data-generation-99-publish_files"
    ],
    "filters": [],
    "includes": {}
  }
}