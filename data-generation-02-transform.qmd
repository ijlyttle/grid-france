---
title: Generation Transform
---

The goal of this pipeline is to update a parquet file with generation data from the French grid.

In the **transform** step:

- Parse response, concatenate to existing data, write to parquet files for both "standard" and "fake UTC" timestamps

```{python}
import os
import dvc.api
import json
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import requests
import functools, itertools
import polars as pl
from pyprojroot.here import here

```

First, parameters:

```{python}
params = dvc.api.params_show("dvc-params.yaml")
```

Let's load in the results from the previous step:

```{python}
with open(here("data/01-ingest/generation.json"), "r") as file:
    array = json.load(file)

table_existing = pl.read_parquet(here("data/99-publish/standard/generation.parquet"))
```

Here, I use some functional-programming tools (because that's what I know) to make one observation per interval and production-type:

```{python}
by_type = list(
    map(
        lambda x: list(
            map(
                lambda v: {
                    "type": x["production_type"],
                    "interval_start": v["start_date"],
                    "interval_end": v["end_date"],
                    "generation": v["value"],
                },
                x["values"],
            )
        ),
        array,
    )
)

fixed = functools.reduce(itertools.chain, by_type)
```

We're now in a form to convert this to a Polars DataFrame, to parse, etc.

```{python}
table_raw = pl.DataFrame(fixed)
```

```{python}
table = table_raw.with_columns(
    [
        pl.col("interval_start")
        .str.strptime(pl.Datetime("ms"))
        .dt.convert_time_zone(time_zone=params["tz_local"]),
        pl.col("interval_end")
        .str.strptime(pl.Datetime("ms"))
        .dt.convert_time_zone(time_zone=params["tz_local"]),
    ]
)
```


```{python}
table_combined = (
    pl.concat([table_existing, table])
    .unique(subset=["interval_start", "type"])
    .sort(by=["interval_start", "type"])
)
```


For the combined table, for each type of generation, let's see how many observations and null values are in the table

```{python}
table_combined.groupby(pl.col("type")).agg(
    pl.col("interval_start").min(),
    pl.col("interval_end").max(),
    pl.col("generation").count().alias("n_observations"),
    pl.col("generation").null_count().alias("n_observations_null"),
)
```

Finally, write the result:

```{python}
table_combined.write_parquet(here("data/02-transform/generation.parquet"))
```




